{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN[Github]",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbIS6VTV-nUX",
        "colab_type": "text"
      },
      "source": [
        "# Deep Graph Infomax\n",
        "An implementation of [_Deep Graph Infomax Velickovic et al._](https://arxiv.org/abs/1809.10341) with pyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aQ3SgUx7-fV",
        "colab_type": "code",
        "outputId": "1b7d7164-c165-4ce7-e837-814d96dd5d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/mehdibalouchi/GNN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'GNN' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af-7wOwh7-gA",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions\n",
        "We first implement some helper functions for later usage. Each function has a `docstring` that provides information about its usage and arguments.\n",
        "\n",
        "_**Note:** `load_data` is written based on Cora dataset provided by Thomas Kipf and may not work with other datasets._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G7-hYqB1O6Hm",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def parse_skipgram(fname):\n",
        "    with open(fname) as f:\n",
        "        toks = list(f.read().split())\n",
        "    nb_nodes = int(toks[0])\n",
        "    nb_features = int(toks[1])\n",
        "    ret = np.empty((nb_nodes, nb_features))\n",
        "    it = 2\n",
        "    for i in range(nb_nodes):\n",
        "        cur_nd = int(toks[it]) - 1\n",
        "        it += 1\n",
        "        for j in range(nb_features):\n",
        "            cur_ft = float(toks[it])\n",
        "            ret[cur_nd][j] = cur_ft\n",
        "            it += 1\n",
        "    return ret\n",
        "\n",
        "# Process a (subset of) a TU dataset into standard form\n",
        "def process_tu(data, nb_nodes):\n",
        "    nb_graphs = len(data)\n",
        "    ft_size = data.num_features\n",
        "\n",
        "    features = np.zeros((nb_graphs, nb_nodes, ft_size))\n",
        "    adjacency = np.zeros((nb_graphs, nb_nodes, nb_nodes))\n",
        "    labels = np.zeros(nb_graphs)\n",
        "    sizes = np.zeros(nb_graphs, dtype=np.int32)\n",
        "    masks = np.zeros((nb_graphs, nb_nodes))\n",
        "       \n",
        "    for g in range(nb_graphs):\n",
        "        sizes[g] = data[g].x.shape[0]\n",
        "        features[g, :sizes[g]] = data[g].x\n",
        "        labels[g] = data[g].y[0]\n",
        "        masks[g, :sizes[g]] = 1.0\n",
        "        e_ind = data[g].edge_index\n",
        "        coo = sp.coo_matrix((np.ones(e_ind.shape[1]), (e_ind[0, :], e_ind[1, :])), shape=(nb_nodes, nb_nodes))\n",
        "        adjacency[g] = coo.todense()\n",
        "\n",
        "    return features, adjacency, labels, sizes, masks\n",
        "\n",
        "def micro_f1(logits, labels):\n",
        "    # Compute predictions\n",
        "    preds = torch.round(nn.Sigmoid()(logits))\n",
        "    \n",
        "    # Cast to avoid trouble\n",
        "    preds = preds.long()\n",
        "    labels = labels.long()\n",
        "\n",
        "    # Count true positives, true negatives, false positives, false negatives\n",
        "    tp = torch.nonzero(preds * labels).shape[0] * 1.0\n",
        "    tn = torch.nonzero((preds - 1) * (labels - 1)).shape[0] * 1.0\n",
        "    fp = torch.nonzero(preds * (labels - 1)).shape[0] * 1.0\n",
        "    fn = torch.nonzero((preds - 1) * labels).shape[0] * 1.0\n",
        "\n",
        "    # Compute micro-f1 score\n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp / (tp + fn)\n",
        "    f1 = (2 * prec * rec) / (prec + rec)\n",
        "    return f1\n",
        "\n",
        "\"\"\"\n",
        " Prepare adjacency matrix by expanding up to a given neighbourhood.\n",
        " This will insert loops on every node.\n",
        " Finally, the matrix is converted to bias vectors.\n",
        " Expected shape: [graph, nodes, nodes]\n",
        "\"\"\"\n",
        "def adj_to_bias(adj, sizes, nhood=1):\n",
        "    nb_graphs = adj.shape[0]\n",
        "    mt = np.empty(adj.shape)\n",
        "    for g in range(nb_graphs):\n",
        "        mt[g] = np.eye(adj.shape[1])\n",
        "        for _ in range(nhood):\n",
        "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n",
        "        for i in range(sizes[g]):\n",
        "            for j in range(sizes[g]):\n",
        "                if mt[g][i][j] > 0.0:\n",
        "                    mt[g][i][j] = 1.0\n",
        "    return -1e9 * (1.0 - mt)\n",
        "\n",
        "\n",
        "###############################################\n",
        "# This section of code adapted from tkipf/gcn #\n",
        "###############################################\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "def sample_mask(idx, l):\n",
        "    \"\"\"Create mask.\"\"\"\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "def load_data(dataset_str): # {'pubmed', 'citeseer', 'cora'}\n",
        "    \"\"\"Load data.\"\"\"\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"./GNN/data/dgi/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(\"./GNN/data/dgi/ind.{}.test.index\".format(dataset_str))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    if dataset_str == 'citeseer':\n",
        "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "        # Find isolated nodes, add them as zero-vecs into the right position\n",
        "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "        tx = tx_extended\n",
        "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
        "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
        "        ty = ty_extended\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "\n",
        "    labels = np.vstack((ally, ty))\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "\n",
        "    idx_test = test_idx_range.tolist()\n",
        "    idx_train = range(len(y))\n",
        "    idx_val = range(len(y), len(y)+500)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "def sparse_to_tuple(sparse_mx, insert_batch=False):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    \"\"\"Set insert_batch=True if you want to insert a batch dimension.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        if insert_batch:\n",
        "            coords = np.vstack((np.zeros(mx.row.shape[0]), mx.row, mx.col)).transpose()\n",
        "            values = mx.data\n",
        "            shape = (1,) + mx.shape\n",
        "        else:\n",
        "            coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "            values = mx.data\n",
        "            shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "def standardize_data(f, train_mask):\n",
        "    \"\"\"Standardize feature matrix and convert to tuple representation\"\"\"\n",
        "    # standardize data\n",
        "    f = f.todense()\n",
        "    mu = f[train_mask == True, :].mean(axis=0)\n",
        "    sigma = f[train_mask == True, :].std(axis=0)\n",
        "    f = f[:, np.squeeze(np.array(sigma > 0))]\n",
        "    mu = f[train_mask == True, :].mean(axis=0)\n",
        "    sigma = f[train_mask == True, :].std(axis=0)\n",
        "    f = (f - mu) / sigma\n",
        "    return f\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return features.todense(), sparse_to_tuple(features)\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2wkxjDjkPNST"
      },
      "source": [
        "## Layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XhAByHStPZrp",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_ft, out_ft, act, bias=True):\n",
        "        super(GCN, self).__init__()\n",
        "        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n",
        "        self.act = nn.PReLU() if act == 'prelu' else act\n",
        "        \n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n",
        "            self.bias.data.fill_(0.0)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    # Shape of seq: (batch, nodes, features)\n",
        "    def forward(self, seq, adj, sparse=False):\n",
        "        seq_fts = self.fc(seq)\n",
        "        if sparse:\n",
        "            out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0)\n",
        "        else:\n",
        "            out = torch.bmm(adj, seq_fts)\n",
        "        if self.bias is not None:\n",
        "            out += self.bias\n",
        "        \n",
        "        return self.act(out)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_h):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Bilinear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
        "        c_x = torch.unsqueeze(c, 1)\n",
        "        c_x = c_x.expand_as(h_pl)\n",
        "\n",
        "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n",
        "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n",
        "\n",
        "        if s_bias1 is not None:\n",
        "            sc_1 += s_bias1\n",
        "        if s_bias2 is not None:\n",
        "            sc_2 += s_bias2\n",
        "\n",
        "        logits = torch.cat((sc_1, sc_2), 1)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Applies an average on seq, of shape (batch, nodes, features)\n",
        "# While taking into account the masking of msk\n",
        "class AvgReadout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AvgReadout, self).__init__()\n",
        "\n",
        "    def forward(self, seq, msk):\n",
        "        if msk is None:\n",
        "            return torch.mean(seq, 1)\n",
        "        else:\n",
        "            msk = torch.unsqueeze(msk, -1)\n",
        "            return torch.sum(seq * msk, 1) / torch.sum(msk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gxrDpCzVPeUu"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2gVGNZBEPi1w",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DGI(nn.Module):\n",
        "    def __init__(self, n_in, n_h, activation):\n",
        "        super(DGI, self).__init__()\n",
        "        self.gcn = GCN(n_in, n_h, activation)\n",
        "        self.read = AvgReadout()\n",
        "\n",
        "        self.sigm = nn.Sigmoid()\n",
        "\n",
        "        self.disc = Discriminator(n_h)\n",
        "\n",
        "    def forward(self, seq1, seq2, adj, sparse, msk, samp_bias1, samp_bias2):\n",
        "        h_1 = self.gcn(seq1, adj, sparse)\n",
        "\n",
        "        c = self.read(h_1, msk)\n",
        "        c = self.sigm(c)\n",
        "\n",
        "        h_2 = self.gcn(seq2, adj, sparse)\n",
        "\n",
        "        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    # Detach the return variables\n",
        "    def embed(self, seq, adj, sparse, msk):\n",
        "        h_1 = self.gcn(seq, adj, sparse)\n",
        "        c = self.read(h_1, msk)\n",
        "\n",
        "        return h_1.detach(), c.detach()\n",
        "\n",
        "\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self, ft_in, nb_classes):\n",
        "        super(LogReg, self).__init__()\n",
        "        self.fc = nn.Linear(ft_in, nb_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        ret = self.fc(seq)\n",
        "        return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eUPU9gaEPtMe"
      },
      "source": [
        "## Train, Validate and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EniVW8VVP-XF",
        "outputId": "164abd78-ded1-4f4a-e269-29b07ca84ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "dataset = 'cora'\n",
        "\n",
        "# training params\n",
        "batch_size = 1\n",
        "nb_epochs = 10000\n",
        "patience = 20\n",
        "lr = 0.001\n",
        "l2_coef = 0.0\n",
        "drop_prob = 0.0\n",
        "hid_units = 512\n",
        "sparse = True\n",
        "nonlinearity = 'prelu' # special name to separate parameters\n",
        "\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data(dataset)\n",
        "features, _ = preprocess_features(features)\n",
        "\n",
        "nb_nodes = features.shape[0]\n",
        "ft_size = features.shape[1]\n",
        "nb_classes = labels.shape[1]\n",
        "\n",
        "adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "if sparse:\n",
        "    sp_adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "else:\n",
        "    adj = (adj + sp.eye(adj.shape[0])).todense()\n",
        "\n",
        "features = torch.FloatTensor(features[np.newaxis])\n",
        "if not sparse:\n",
        "    adj = torch.FloatTensor(adj[np.newaxis])\n",
        "labels = torch.FloatTensor(labels[np.newaxis])\n",
        "idx_train = torch.LongTensor(idx_train)\n",
        "idx_val = torch.LongTensor(idx_val)\n",
        "idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "model = DGI(ft_size, hid_units, nonlinearity)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('Using CUDA')\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    if sparse:\n",
        "        sp_adj = sp_adj.cuda()\n",
        "    else:\n",
        "        adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "b_xent = nn.BCEWithLogitsLoss()\n",
        "xent = nn.CrossEntropyLoss()\n",
        "cnt_wait = 0\n",
        "best = 1e9\n",
        "best_t = 0\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    model.train()\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    idx = np.random.permutation(nb_nodes)\n",
        "    shuf_fts = features[:, idx, :]\n",
        "\n",
        "    lbl_1 = torch.ones(batch_size, nb_nodes)\n",
        "    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
        "    lbl = torch.cat((lbl_1, lbl_2), 1)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        shuf_fts = shuf_fts.cuda()\n",
        "        lbl = lbl.cuda()\n",
        "    \n",
        "    logits = model(features, shuf_fts, sp_adj if sparse else adj, sparse, None, None, None) \n",
        "\n",
        "    loss = b_xent(logits, lbl)\n",
        "\n",
        "    print('Loss:', loss)\n",
        "\n",
        "    if loss < best:\n",
        "        best = loss\n",
        "        best_t = epoch\n",
        "        cnt_wait = 0\n",
        "        torch.save(model.state_dict(), 'best_dgi.pkl')\n",
        "    else:\n",
        "        cnt_wait += 1\n",
        "\n",
        "    if cnt_wait == patience:\n",
        "        print('Early stopping!')\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "print('Loading {}th epoch'.format(best_t))\n",
        "model.load_state_dict(torch.load('best_dgi.pkl'))\n",
        "\n",
        "embeds, _ = model.embed(features, sp_adj if sparse else adj, sparse, None)\n",
        "train_embs = embeds[0, idx_train]\n",
        "val_embs = embeds[0, idx_val]\n",
        "test_embs = embeds[0, idx_test]\n",
        "\n",
        "train_lbls = torch.argmax(labels[0, idx_train], dim=1)\n",
        "val_lbls = torch.argmax(labels[0, idx_val], dim=1)\n",
        "test_lbls = torch.argmax(labels[0, idx_test], dim=1)\n",
        "\n",
        "tot = torch.zeros(1)\n",
        "tot = tot.cuda()\n",
        "\n",
        "accs = []\n",
        "\n",
        "for _ in range(50):\n",
        "    log = LogReg(hid_units, nb_classes)\n",
        "    opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
        "    log.cuda()\n",
        "\n",
        "    pat_steps = 0\n",
        "    best_acc = torch.zeros(1)\n",
        "    best_acc = best_acc.cuda()\n",
        "    for _ in range(100):\n",
        "        log.train()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        logits = log(train_embs)\n",
        "        loss = xent(logits, train_lbls)\n",
        "        \n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    logits = log(test_embs)\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]\n",
        "    accs.append(acc * 100)\n",
        "    print(acc)\n",
        "    tot += acc\n",
        "\n",
        "print('Average accuracy:', tot / 50)\n",
        "\n",
        "accs = torch.stack(accs)\n",
        "print(accs.mean())\n",
        "print(accs.std())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.8939 acc_train: 0.2643 loss_val: 1.8806 acc_val: 0.3167 time: 0.0075s\n",
            "Epoch: 0002 loss_train: 1.8803 acc_train: 0.3214 loss_val: 1.8704 acc_val: 0.3933 time: 0.0062s\n",
            "Epoch: 0003 loss_train: 1.8641 acc_train: 0.3571 loss_val: 1.8602 acc_val: 0.3633 time: 0.0062s\n",
            "Epoch: 0004 loss_train: 1.8672 acc_train: 0.3286 loss_val: 1.8502 acc_val: 0.3533 time: 0.0061s\n",
            "Epoch: 0005 loss_train: 1.8582 acc_train: 0.3286 loss_val: 1.8405 acc_val: 0.3500 time: 0.0060s\n",
            "Epoch: 0006 loss_train: 1.8479 acc_train: 0.3143 loss_val: 1.8310 acc_val: 0.3500 time: 0.0064s\n",
            "Epoch: 0007 loss_train: 1.8199 acc_train: 0.3143 loss_val: 1.8217 acc_val: 0.3500 time: 0.0063s\n",
            "Epoch: 0008 loss_train: 1.8229 acc_train: 0.3000 loss_val: 1.8127 acc_val: 0.3500 time: 0.0062s\n",
            "Epoch: 0009 loss_train: 1.8199 acc_train: 0.3000 loss_val: 1.8041 acc_val: 0.3500 time: 0.0061s\n",
            "Epoch: 0010 loss_train: 1.7921 acc_train: 0.3071 loss_val: 1.7955 acc_val: 0.3500 time: 0.0063s\n",
            "Epoch: 0011 loss_train: 1.8046 acc_train: 0.2857 loss_val: 1.7873 acc_val: 0.3500 time: 0.0063s\n",
            "Epoch: 0012 loss_train: 1.7857 acc_train: 0.2857 loss_val: 1.7792 acc_val: 0.3500 time: 0.0062s\n",
            "Epoch: 0013 loss_train: 1.7801 acc_train: 0.2857 loss_val: 1.7716 acc_val: 0.3500 time: 0.0076s\n",
            "Epoch: 0014 loss_train: 1.7762 acc_train: 0.2929 loss_val: 1.7642 acc_val: 0.3500 time: 0.0069s\n",
            "Epoch: 0015 loss_train: 1.7693 acc_train: 0.2929 loss_val: 1.7570 acc_val: 0.3500 time: 0.0062s\n",
            "Epoch: 0016 loss_train: 1.7562 acc_train: 0.2929 loss_val: 1.7500 acc_val: 0.3500 time: 0.0062s\n",
            "Epoch: 0017 loss_train: 1.7491 acc_train: 0.2929 loss_val: 1.7434 acc_val: 0.3500 time: 0.0062s\n",
            "Epoch: 0018 loss_train: 1.7229 acc_train: 0.2929 loss_val: 1.7369 acc_val: 0.3500 time: 0.0064s\n",
            "Epoch: 0019 loss_train: 1.7401 acc_train: 0.3143 loss_val: 1.7306 acc_val: 0.3500 time: 0.0062s\n",
            "Epoch: 0020 loss_train: 1.7421 acc_train: 0.3143 loss_val: 1.7245 acc_val: 0.3500 time: 0.0065s\n",
            "Epoch: 0021 loss_train: 1.7031 acc_train: 0.3000 loss_val: 1.7185 acc_val: 0.3500 time: 0.0062s\n",
            "Epoch: 0022 loss_train: 1.6696 acc_train: 0.3214 loss_val: 1.7123 acc_val: 0.3500 time: 0.0091s\n",
            "Epoch: 0023 loss_train: 1.6813 acc_train: 0.3000 loss_val: 1.7060 acc_val: 0.3500 time: 0.0073s\n",
            "Epoch: 0024 loss_train: 1.6865 acc_train: 0.3214 loss_val: 1.6996 acc_val: 0.3533 time: 0.0090s\n",
            "Epoch: 0025 loss_train: 1.6916 acc_train: 0.3214 loss_val: 1.6932 acc_val: 0.3533 time: 0.0062s\n",
            "Epoch: 0026 loss_train: 1.6563 acc_train: 0.3500 loss_val: 1.6866 acc_val: 0.3567 time: 0.0063s\n",
            "Epoch: 0027 loss_train: 1.6460 acc_train: 0.3357 loss_val: 1.6796 acc_val: 0.3600 time: 0.0064s\n",
            "Epoch: 0028 loss_train: 1.6465 acc_train: 0.3571 loss_val: 1.6725 acc_val: 0.3667 time: 0.0062s\n",
            "Epoch: 0029 loss_train: 1.6436 acc_train: 0.3643 loss_val: 1.6648 acc_val: 0.3667 time: 0.0118s\n",
            "Epoch: 0030 loss_train: 1.6452 acc_train: 0.3643 loss_val: 1.6570 acc_val: 0.3700 time: 0.0072s\n",
            "Epoch: 0031 loss_train: 1.5932 acc_train: 0.4143 loss_val: 1.6484 acc_val: 0.3700 time: 0.0063s\n",
            "Epoch: 0032 loss_train: 1.5870 acc_train: 0.4071 loss_val: 1.6392 acc_val: 0.3800 time: 0.0062s\n",
            "Epoch: 0033 loss_train: 1.5902 acc_train: 0.4000 loss_val: 1.6296 acc_val: 0.3867 time: 0.0061s\n",
            "Epoch: 0034 loss_train: 1.5662 acc_train: 0.4286 loss_val: 1.6197 acc_val: 0.3933 time: 0.0062s\n",
            "Epoch: 0035 loss_train: 1.5485 acc_train: 0.4643 loss_val: 1.6095 acc_val: 0.4000 time: 0.0063s\n",
            "Epoch: 0036 loss_train: 1.5589 acc_train: 0.4214 loss_val: 1.5987 acc_val: 0.4033 time: 0.0062s\n",
            "Epoch: 0037 loss_train: 1.5351 acc_train: 0.4357 loss_val: 1.5876 acc_val: 0.4033 time: 0.0062s\n",
            "Epoch: 0038 loss_train: 1.4956 acc_train: 0.4500 loss_val: 1.5762 acc_val: 0.4100 time: 0.0062s\n",
            "Epoch: 0039 loss_train: 1.5021 acc_train: 0.4500 loss_val: 1.5647 acc_val: 0.4100 time: 0.0061s\n",
            "Epoch: 0040 loss_train: 1.4860 acc_train: 0.4643 loss_val: 1.5532 acc_val: 0.4100 time: 0.0063s\n",
            "Epoch: 0041 loss_train: 1.4875 acc_train: 0.4786 loss_val: 1.5414 acc_val: 0.4100 time: 0.0065s\n",
            "Epoch: 0042 loss_train: 1.4533 acc_train: 0.4500 loss_val: 1.5297 acc_val: 0.4067 time: 0.0062s\n",
            "Epoch: 0043 loss_train: 1.4276 acc_train: 0.4786 loss_val: 1.5180 acc_val: 0.4167 time: 0.0063s\n",
            "Epoch: 0044 loss_train: 1.4423 acc_train: 0.4929 loss_val: 1.5065 acc_val: 0.4367 time: 0.0063s\n",
            "Epoch: 0045 loss_train: 1.4007 acc_train: 0.4786 loss_val: 1.4945 acc_val: 0.4567 time: 0.0061s\n",
            "Epoch: 0046 loss_train: 1.3833 acc_train: 0.5000 loss_val: 1.4822 acc_val: 0.4733 time: 0.0062s\n",
            "Epoch: 0047 loss_train: 1.3836 acc_train: 0.5357 loss_val: 1.4698 acc_val: 0.5067 time: 0.0062s\n",
            "Epoch: 0048 loss_train: 1.3585 acc_train: 0.5857 loss_val: 1.4573 acc_val: 0.5167 time: 0.0061s\n",
            "Epoch: 0049 loss_train: 1.3470 acc_train: 0.5857 loss_val: 1.4446 acc_val: 0.5300 time: 0.0061s\n",
            "Epoch: 0050 loss_train: 1.3404 acc_train: 0.5643 loss_val: 1.4321 acc_val: 0.5367 time: 0.0059s\n",
            "Epoch: 0051 loss_train: 1.2987 acc_train: 0.5857 loss_val: 1.4195 acc_val: 0.5533 time: 0.0054s\n",
            "Epoch: 0052 loss_train: 1.3035 acc_train: 0.6000 loss_val: 1.4072 acc_val: 0.5600 time: 0.0052s\n",
            "Epoch: 0053 loss_train: 1.2766 acc_train: 0.6143 loss_val: 1.3942 acc_val: 0.5733 time: 0.0053s\n",
            "Epoch: 0054 loss_train: 1.2799 acc_train: 0.6071 loss_val: 1.3808 acc_val: 0.5767 time: 0.0053s\n",
            "Epoch: 0055 loss_train: 1.2316 acc_train: 0.6357 loss_val: 1.3675 acc_val: 0.5833 time: 0.0052s\n",
            "Epoch: 0056 loss_train: 1.2471 acc_train: 0.6357 loss_val: 1.3546 acc_val: 0.6000 time: 0.0053s\n",
            "Epoch: 0057 loss_train: 1.2224 acc_train: 0.6214 loss_val: 1.3415 acc_val: 0.6100 time: 0.0052s\n",
            "Epoch: 0058 loss_train: 1.2030 acc_train: 0.6357 loss_val: 1.3287 acc_val: 0.6233 time: 0.0054s\n",
            "Epoch: 0059 loss_train: 1.1717 acc_train: 0.6643 loss_val: 1.3162 acc_val: 0.6233 time: 0.0081s\n",
            "Epoch: 0060 loss_train: 1.1494 acc_train: 0.6929 loss_val: 1.3040 acc_val: 0.6333 time: 0.0067s\n",
            "Epoch: 0061 loss_train: 1.1762 acc_train: 0.6357 loss_val: 1.2919 acc_val: 0.6433 time: 0.0057s\n",
            "Epoch: 0062 loss_train: 1.0863 acc_train: 0.6857 loss_val: 1.2797 acc_val: 0.6533 time: 0.0059s\n",
            "Epoch: 0063 loss_train: 1.0930 acc_train: 0.6857 loss_val: 1.2677 acc_val: 0.6633 time: 0.0058s\n",
            "Epoch: 0064 loss_train: 1.0923 acc_train: 0.6929 loss_val: 1.2559 acc_val: 0.6667 time: 0.0061s\n",
            "Epoch: 0065 loss_train: 1.0899 acc_train: 0.6929 loss_val: 1.2440 acc_val: 0.6633 time: 0.0057s\n",
            "Epoch: 0066 loss_train: 1.0881 acc_train: 0.6857 loss_val: 1.2320 acc_val: 0.6700 time: 0.0057s\n",
            "Epoch: 0067 loss_train: 1.0989 acc_train: 0.6857 loss_val: 1.2201 acc_val: 0.6667 time: 0.0057s\n",
            "Epoch: 0068 loss_train: 1.0418 acc_train: 0.7143 loss_val: 1.2075 acc_val: 0.6667 time: 0.0052s\n",
            "Epoch: 0069 loss_train: 1.0045 acc_train: 0.7000 loss_val: 1.1951 acc_val: 0.6700 time: 0.0051s\n",
            "Epoch: 0070 loss_train: 0.9929 acc_train: 0.7357 loss_val: 1.1832 acc_val: 0.6700 time: 0.0050s\n",
            "Epoch: 0071 loss_train: 0.9822 acc_train: 0.7000 loss_val: 1.1719 acc_val: 0.6767 time: 0.0051s\n",
            "Epoch: 0072 loss_train: 1.0159 acc_train: 0.7571 loss_val: 1.1609 acc_val: 0.6833 time: 0.0051s\n",
            "Epoch: 0073 loss_train: 1.0309 acc_train: 0.6857 loss_val: 1.1501 acc_val: 0.7000 time: 0.0052s\n",
            "Epoch: 0074 loss_train: 0.9354 acc_train: 0.7286 loss_val: 1.1396 acc_val: 0.7000 time: 0.0053s\n",
            "Epoch: 0075 loss_train: 0.9533 acc_train: 0.7357 loss_val: 1.1297 acc_val: 0.7133 time: 0.0051s\n",
            "Epoch: 0076 loss_train: 0.9670 acc_train: 0.7571 loss_val: 1.1202 acc_val: 0.7200 time: 0.0052s\n",
            "Epoch: 0077 loss_train: 0.9410 acc_train: 0.7643 loss_val: 1.1109 acc_val: 0.7233 time: 0.0052s\n",
            "Epoch: 0078 loss_train: 0.8936 acc_train: 0.8000 loss_val: 1.1017 acc_val: 0.7233 time: 0.0052s\n",
            "Epoch: 0079 loss_train: 0.9356 acc_train: 0.7500 loss_val: 1.0929 acc_val: 0.7233 time: 0.0054s\n",
            "Epoch: 0080 loss_train: 0.9025 acc_train: 0.7929 loss_val: 1.0842 acc_val: 0.7233 time: 0.0051s\n",
            "Epoch: 0081 loss_train: 0.9070 acc_train: 0.7500 loss_val: 1.0754 acc_val: 0.7267 time: 0.0052s\n",
            "Epoch: 0082 loss_train: 0.9093 acc_train: 0.7643 loss_val: 1.0671 acc_val: 0.7267 time: 0.0060s\n",
            "Epoch: 0083 loss_train: 0.8404 acc_train: 0.8000 loss_val: 1.0595 acc_val: 0.7300 time: 0.0051s\n",
            "Epoch: 0084 loss_train: 0.8718 acc_train: 0.7786 loss_val: 1.0520 acc_val: 0.7333 time: 0.0052s\n",
            "Epoch: 0085 loss_train: 0.8575 acc_train: 0.7643 loss_val: 1.0449 acc_val: 0.7533 time: 0.0052s\n",
            "Epoch: 0086 loss_train: 0.8493 acc_train: 0.7714 loss_val: 1.0381 acc_val: 0.7600 time: 0.0054s\n",
            "Epoch: 0087 loss_train: 0.8413 acc_train: 0.7714 loss_val: 1.0319 acc_val: 0.7667 time: 0.0055s\n",
            "Epoch: 0088 loss_train: 0.8156 acc_train: 0.8071 loss_val: 1.0252 acc_val: 0.7733 time: 0.0051s\n",
            "Epoch: 0089 loss_train: 0.8291 acc_train: 0.8286 loss_val: 1.0181 acc_val: 0.7833 time: 0.0056s\n",
            "Epoch: 0090 loss_train: 0.7836 acc_train: 0.8000 loss_val: 1.0113 acc_val: 0.7767 time: 0.0052s\n",
            "Epoch: 0091 loss_train: 0.7654 acc_train: 0.8429 loss_val: 1.0056 acc_val: 0.7733 time: 0.0052s\n",
            "Epoch: 0092 loss_train: 0.7945 acc_train: 0.8500 loss_val: 0.9992 acc_val: 0.7733 time: 0.0052s\n",
            "Epoch: 0093 loss_train: 0.7763 acc_train: 0.8357 loss_val: 0.9927 acc_val: 0.7733 time: 0.0080s\n",
            "Epoch: 0094 loss_train: 0.7867 acc_train: 0.8643 loss_val: 0.9856 acc_val: 0.7833 time: 0.0055s\n",
            "Epoch: 0095 loss_train: 0.7750 acc_train: 0.8500 loss_val: 0.9783 acc_val: 0.7867 time: 0.0051s\n",
            "Epoch: 0096 loss_train: 0.7780 acc_train: 0.8429 loss_val: 0.9706 acc_val: 0.7900 time: 0.0052s\n",
            "Epoch: 0097 loss_train: 0.7630 acc_train: 0.8286 loss_val: 0.9635 acc_val: 0.8000 time: 0.0052s\n",
            "Epoch: 0098 loss_train: 0.7258 acc_train: 0.8286 loss_val: 0.9567 acc_val: 0.8033 time: 0.0052s\n",
            "Epoch: 0099 loss_train: 0.7651 acc_train: 0.8429 loss_val: 0.9496 acc_val: 0.8000 time: 0.0053s\n",
            "Epoch: 0100 loss_train: 0.7528 acc_train: 0.8643 loss_val: 0.9434 acc_val: 0.8000 time: 0.0051s\n",
            "Epoch: 0101 loss_train: 0.7265 acc_train: 0.8286 loss_val: 0.9383 acc_val: 0.7900 time: 0.0050s\n",
            "Epoch: 0102 loss_train: 0.7453 acc_train: 0.8643 loss_val: 0.9325 acc_val: 0.7867 time: 0.0052s\n",
            "Epoch: 0103 loss_train: 0.6692 acc_train: 0.8571 loss_val: 0.9269 acc_val: 0.7900 time: 0.0051s\n",
            "Epoch: 0104 loss_train: 0.7230 acc_train: 0.8643 loss_val: 0.9214 acc_val: 0.7933 time: 0.0055s\n",
            "Epoch: 0105 loss_train: 0.7041 acc_train: 0.8857 loss_val: 0.9162 acc_val: 0.7900 time: 0.0051s\n",
            "Epoch: 0106 loss_train: 0.6890 acc_train: 0.8714 loss_val: 0.9111 acc_val: 0.7833 time: 0.0051s\n",
            "Epoch: 0107 loss_train: 0.6906 acc_train: 0.8429 loss_val: 0.9063 acc_val: 0.7833 time: 0.0051s\n",
            "Epoch: 0108 loss_train: 0.6722 acc_train: 0.8357 loss_val: 0.9011 acc_val: 0.7900 time: 0.0055s\n",
            "Epoch: 0109 loss_train: 0.6801 acc_train: 0.8714 loss_val: 0.8956 acc_val: 0.7967 time: 0.0059s\n",
            "Epoch: 0110 loss_train: 0.6610 acc_train: 0.8429 loss_val: 0.8909 acc_val: 0.8000 time: 0.0056s\n",
            "Epoch: 0111 loss_train: 0.6746 acc_train: 0.8429 loss_val: 0.8870 acc_val: 0.8000 time: 0.0056s\n",
            "Epoch: 0112 loss_train: 0.6737 acc_train: 0.8357 loss_val: 0.8834 acc_val: 0.7967 time: 0.0056s\n",
            "Epoch: 0113 loss_train: 0.6127 acc_train: 0.8929 loss_val: 0.8803 acc_val: 0.7933 time: 0.0057s\n",
            "Epoch: 0114 loss_train: 0.6107 acc_train: 0.8786 loss_val: 0.8774 acc_val: 0.7867 time: 0.0054s\n",
            "Epoch: 0115 loss_train: 0.6191 acc_train: 0.8714 loss_val: 0.8739 acc_val: 0.7867 time: 0.0056s\n",
            "Epoch: 0116 loss_train: 0.6456 acc_train: 0.8571 loss_val: 0.8692 acc_val: 0.7900 time: 0.0056s\n",
            "Epoch: 0117 loss_train: 0.6427 acc_train: 0.8786 loss_val: 0.8638 acc_val: 0.7900 time: 0.0056s\n",
            "Epoch: 0118 loss_train: 0.6263 acc_train: 0.9071 loss_val: 0.8587 acc_val: 0.7933 time: 0.0065s\n",
            "Epoch: 0119 loss_train: 0.6052 acc_train: 0.8571 loss_val: 0.8540 acc_val: 0.7900 time: 0.0066s\n",
            "Epoch: 0120 loss_train: 0.6119 acc_train: 0.8857 loss_val: 0.8497 acc_val: 0.8067 time: 0.0068s\n",
            "Epoch: 0121 loss_train: 0.6109 acc_train: 0.8929 loss_val: 0.8466 acc_val: 0.8067 time: 0.0067s\n",
            "Epoch: 0122 loss_train: 0.6008 acc_train: 0.9000 loss_val: 0.8433 acc_val: 0.8067 time: 0.0067s\n",
            "Epoch: 0123 loss_train: 0.5938 acc_train: 0.9214 loss_val: 0.8402 acc_val: 0.8033 time: 0.0067s\n",
            "Epoch: 0124 loss_train: 0.6140 acc_train: 0.9000 loss_val: 0.8370 acc_val: 0.7967 time: 0.0107s\n",
            "Epoch: 0125 loss_train: 0.5772 acc_train: 0.9071 loss_val: 0.8337 acc_val: 0.7967 time: 0.0062s\n",
            "Epoch: 0126 loss_train: 0.5947 acc_train: 0.8857 loss_val: 0.8300 acc_val: 0.8033 time: 0.0060s\n",
            "Epoch: 0127 loss_train: 0.5806 acc_train: 0.8714 loss_val: 0.8266 acc_val: 0.8033 time: 0.0062s\n",
            "Epoch: 0128 loss_train: 0.5796 acc_train: 0.8929 loss_val: 0.8231 acc_val: 0.8033 time: 0.0060s\n",
            "Epoch: 0129 loss_train: 0.5676 acc_train: 0.8929 loss_val: 0.8196 acc_val: 0.8033 time: 0.0062s\n",
            "Epoch: 0130 loss_train: 0.5450 acc_train: 0.9071 loss_val: 0.8155 acc_val: 0.8067 time: 0.0063s\n",
            "Epoch: 0131 loss_train: 0.5666 acc_train: 0.9071 loss_val: 0.8110 acc_val: 0.8067 time: 0.0059s\n",
            "Epoch: 0132 loss_train: 0.5747 acc_train: 0.8786 loss_val: 0.8075 acc_val: 0.8033 time: 0.0060s\n",
            "Epoch: 0133 loss_train: 0.5633 acc_train: 0.9000 loss_val: 0.8041 acc_val: 0.8033 time: 0.0058s\n",
            "Epoch: 0134 loss_train: 0.5555 acc_train: 0.9071 loss_val: 0.8006 acc_val: 0.8067 time: 0.0061s\n",
            "Epoch: 0135 loss_train: 0.5375 acc_train: 0.9214 loss_val: 0.7976 acc_val: 0.8067 time: 0.0060s\n",
            "Epoch: 0136 loss_train: 0.5756 acc_train: 0.9000 loss_val: 0.7965 acc_val: 0.8033 time: 0.0062s\n",
            "Epoch: 0137 loss_train: 0.5545 acc_train: 0.9000 loss_val: 0.7958 acc_val: 0.8067 time: 0.0084s\n",
            "Epoch: 0138 loss_train: 0.5088 acc_train: 0.9000 loss_val: 0.7954 acc_val: 0.8067 time: 0.0075s\n",
            "Epoch: 0139 loss_train: 0.5429 acc_train: 0.8929 loss_val: 0.7949 acc_val: 0.8033 time: 0.0060s\n",
            "Epoch: 0140 loss_train: 0.5600 acc_train: 0.9214 loss_val: 0.7917 acc_val: 0.8067 time: 0.0060s\n",
            "Epoch: 0141 loss_train: 0.5407 acc_train: 0.9143 loss_val: 0.7874 acc_val: 0.8067 time: 0.0062s\n",
            "Epoch: 0142 loss_train: 0.5331 acc_train: 0.9000 loss_val: 0.7836 acc_val: 0.8067 time: 0.0063s\n",
            "Epoch: 0143 loss_train: 0.5310 acc_train: 0.9071 loss_val: 0.7801 acc_val: 0.8100 time: 0.0061s\n",
            "Epoch: 0144 loss_train: 0.5050 acc_train: 0.9143 loss_val: 0.7774 acc_val: 0.8167 time: 0.0062s\n",
            "Epoch: 0145 loss_train: 0.5200 acc_train: 0.8857 loss_val: 0.7754 acc_val: 0.8133 time: 0.0059s\n",
            "Epoch: 0146 loss_train: 0.4931 acc_train: 0.9000 loss_val: 0.7744 acc_val: 0.8167 time: 0.0061s\n",
            "Epoch: 0147 loss_train: 0.5324 acc_train: 0.9000 loss_val: 0.7738 acc_val: 0.8167 time: 0.0060s\n",
            "Epoch: 0148 loss_train: 0.4934 acc_train: 0.9357 loss_val: 0.7728 acc_val: 0.8233 time: 0.0061s\n",
            "Epoch: 0149 loss_train: 0.5066 acc_train: 0.9071 loss_val: 0.7714 acc_val: 0.8200 time: 0.0061s\n",
            "Epoch: 0150 loss_train: 0.5110 acc_train: 0.9000 loss_val: 0.7691 acc_val: 0.8100 time: 0.0060s\n",
            "Epoch: 0151 loss_train: 0.4884 acc_train: 0.9143 loss_val: 0.7660 acc_val: 0.8067 time: 0.0062s\n",
            "Epoch: 0152 loss_train: 0.5036 acc_train: 0.9143 loss_val: 0.7631 acc_val: 0.8167 time: 0.0061s\n",
            "Epoch: 0153 loss_train: 0.4820 acc_train: 0.9286 loss_val: 0.7612 acc_val: 0.8100 time: 0.0079s\n",
            "Epoch: 0154 loss_train: 0.4900 acc_train: 0.9000 loss_val: 0.7587 acc_val: 0.8100 time: 0.0072s\n",
            "Epoch: 0155 loss_train: 0.5186 acc_train: 0.9143 loss_val: 0.7563 acc_val: 0.8100 time: 0.0061s\n",
            "Epoch: 0156 loss_train: 0.4546 acc_train: 0.9286 loss_val: 0.7547 acc_val: 0.8133 time: 0.0060s\n",
            "Epoch: 0157 loss_train: 0.5025 acc_train: 0.9071 loss_val: 0.7533 acc_val: 0.8167 time: 0.0059s\n",
            "Epoch: 0158 loss_train: 0.4927 acc_train: 0.9214 loss_val: 0.7520 acc_val: 0.8200 time: 0.0059s\n",
            "Epoch: 0159 loss_train: 0.5354 acc_train: 0.9071 loss_val: 0.7512 acc_val: 0.8167 time: 0.0061s\n",
            "Epoch: 0160 loss_train: 0.4590 acc_train: 0.9286 loss_val: 0.7512 acc_val: 0.8167 time: 0.0061s\n",
            "Epoch: 0161 loss_train: 0.4643 acc_train: 0.9143 loss_val: 0.7516 acc_val: 0.8067 time: 0.0061s\n",
            "Epoch: 0162 loss_train: 0.4864 acc_train: 0.9357 loss_val: 0.7521 acc_val: 0.8100 time: 0.0060s\n",
            "Epoch: 0163 loss_train: 0.4339 acc_train: 0.9429 loss_val: 0.7523 acc_val: 0.8033 time: 0.0060s\n",
            "Epoch: 0164 loss_train: 0.4625 acc_train: 0.9143 loss_val: 0.7513 acc_val: 0.7967 time: 0.0060s\n",
            "Epoch: 0165 loss_train: 0.4434 acc_train: 0.9429 loss_val: 0.7492 acc_val: 0.7933 time: 0.0061s\n",
            "Epoch: 0166 loss_train: 0.4688 acc_train: 0.9357 loss_val: 0.7462 acc_val: 0.8000 time: 0.0052s\n",
            "Epoch: 0167 loss_train: 0.4772 acc_train: 0.9286 loss_val: 0.7435 acc_val: 0.8100 time: 0.0051s\n",
            "Epoch: 0168 loss_train: 0.4519 acc_train: 0.9571 loss_val: 0.7405 acc_val: 0.8067 time: 0.0052s\n",
            "Epoch: 0169 loss_train: 0.4242 acc_train: 0.9286 loss_val: 0.7387 acc_val: 0.8067 time: 0.0066s\n",
            "Epoch: 0170 loss_train: 0.4453 acc_train: 0.9500 loss_val: 0.7363 acc_val: 0.8100 time: 0.0067s\n",
            "Epoch: 0171 loss_train: 0.4434 acc_train: 0.9143 loss_val: 0.7343 acc_val: 0.8100 time: 0.0061s\n",
            "Epoch: 0172 loss_train: 0.4416 acc_train: 0.9500 loss_val: 0.7318 acc_val: 0.8100 time: 0.0059s\n",
            "Epoch: 0173 loss_train: 0.4410 acc_train: 0.9429 loss_val: 0.7300 acc_val: 0.8067 time: 0.0099s\n",
            "Epoch: 0174 loss_train: 0.4314 acc_train: 0.9214 loss_val: 0.7304 acc_val: 0.8067 time: 0.0070s\n",
            "Epoch: 0175 loss_train: 0.4519 acc_train: 0.9500 loss_val: 0.7306 acc_val: 0.8033 time: 0.0071s\n",
            "Epoch: 0176 loss_train: 0.5041 acc_train: 0.9071 loss_val: 0.7309 acc_val: 0.8000 time: 0.0063s\n",
            "Epoch: 0177 loss_train: 0.4102 acc_train: 0.9214 loss_val: 0.7309 acc_val: 0.8000 time: 0.0060s\n",
            "Epoch: 0178 loss_train: 0.4768 acc_train: 0.9429 loss_val: 0.7297 acc_val: 0.8000 time: 0.0061s\n",
            "Epoch: 0179 loss_train: 0.4433 acc_train: 0.9071 loss_val: 0.7283 acc_val: 0.8033 time: 0.0058s\n",
            "Epoch: 0180 loss_train: 0.4871 acc_train: 0.9214 loss_val: 0.7261 acc_val: 0.8033 time: 0.0060s\n",
            "Epoch: 0181 loss_train: 0.4536 acc_train: 0.9357 loss_val: 0.7245 acc_val: 0.8067 time: 0.0060s\n",
            "Epoch: 0182 loss_train: 0.4259 acc_train: 0.9143 loss_val: 0.7232 acc_val: 0.8033 time: 0.0066s\n",
            "Epoch: 0183 loss_train: 0.4147 acc_train: 0.9500 loss_val: 0.7217 acc_val: 0.8067 time: 0.0086s\n",
            "Epoch: 0184 loss_train: 0.4234 acc_train: 0.9286 loss_val: 0.7203 acc_val: 0.8067 time: 0.0063s\n",
            "Epoch: 0185 loss_train: 0.4074 acc_train: 0.9500 loss_val: 0.7190 acc_val: 0.8067 time: 0.0062s\n",
            "Epoch: 0186 loss_train: 0.4530 acc_train: 0.9286 loss_val: 0.7186 acc_val: 0.8067 time: 0.0059s\n",
            "Epoch: 0187 loss_train: 0.4487 acc_train: 0.9357 loss_val: 0.7178 acc_val: 0.8133 time: 0.0060s\n",
            "Epoch: 0188 loss_train: 0.4243 acc_train: 0.9357 loss_val: 0.7162 acc_val: 0.8100 time: 0.0060s\n",
            "Epoch: 0189 loss_train: 0.4455 acc_train: 0.9071 loss_val: 0.7151 acc_val: 0.8133 time: 0.0062s\n",
            "Epoch: 0190 loss_train: 0.4041 acc_train: 0.9429 loss_val: 0.7142 acc_val: 0.8100 time: 0.0059s\n",
            "Epoch: 0191 loss_train: 0.4175 acc_train: 0.9429 loss_val: 0.7149 acc_val: 0.8033 time: 0.0060s\n",
            "Epoch: 0192 loss_train: 0.4065 acc_train: 0.9357 loss_val: 0.7150 acc_val: 0.8033 time: 0.0070s\n",
            "Epoch: 0193 loss_train: 0.4088 acc_train: 0.9571 loss_val: 0.7149 acc_val: 0.8000 time: 0.0064s\n",
            "Epoch: 0194 loss_train: 0.4313 acc_train: 0.9429 loss_val: 0.7140 acc_val: 0.8000 time: 0.0083s\n",
            "Epoch: 0195 loss_train: 0.4183 acc_train: 0.9286 loss_val: 0.7124 acc_val: 0.8000 time: 0.0082s\n",
            "Epoch: 0196 loss_train: 0.4039 acc_train: 0.9357 loss_val: 0.7113 acc_val: 0.8000 time: 0.0070s\n",
            "Epoch: 0197 loss_train: 0.4300 acc_train: 0.9286 loss_val: 0.7090 acc_val: 0.8100 time: 0.0069s\n",
            "Epoch: 0198 loss_train: 0.4232 acc_train: 0.9429 loss_val: 0.7072 acc_val: 0.8100 time: 0.0083s\n",
            "Epoch: 0199 loss_train: 0.4279 acc_train: 0.9286 loss_val: 0.7050 acc_val: 0.8133 time: 0.0061s\n",
            "Epoch: 0200 loss_train: 0.3758 acc_train: 0.9500 loss_val: 0.7033 acc_val: 0.8100 time: 0.0064s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.3784s\n",
            "Test set results: loss= 0.7299 accuracy= 0.8230\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}