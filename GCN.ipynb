{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/mehdibalouchi/GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Helper functions\n",
        "We first implement some helper functions for later usage. Each function has a `docstring` that provides information about its usage and arguments.\n",
        "\n",
        "_**Note:** `load_data` is written based on Cora dataset provided by Thomas Kipf and may not work with other datasets._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "G7-hYqB1O6Hm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    \"\"\" \n",
        "    Encode a vector to onehot\n",
        "    \n",
        "    :param labels: vector of labels to be encoded to onehot\n",
        "    :returns: a vector of onehot encoding of each number in `labels`\n",
        "    \"\"\"\n",
        "\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def load_data(path=\"./GNN/data/cora/\", dataset=\"cora\"):\n",
        "    \"\"\"\n",
        "    Load Cora dataset or any other data formatted like Thomas Kipf's dataset.\n",
        "\n",
        "    :param path: path to .content and .cites directory\n",
        "    :param dataset: dataset name _example: cora => cora.content, cora.cites_\n",
        "\n",
        "    :returns: A list containing\n",
        "        [0]: Adjacency matrices,\n",
        "        [1]: Features matrices,\n",
        "        [2]: Labels matrices,\n",
        "        [3]: Train mask,\n",
        "        [4]: Validation mask,\n",
        "        [5]: Test mask\n",
        "    \"\"\"\n",
        "    \n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"\n",
        "    Row-normalize sparse matrix\n",
        "\n",
        "    :param mx: saprse matrix to be row-normalized\n",
        "    :returns: row-normalized sparse matrix\n",
        "    \"\"\"\n",
        "\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    \"\"\"\n",
        "    Calculate accuracy given true labels and model output (rate of correct predictions)\n",
        "\n",
        "    :param output: Model's output\n",
        "    :param labels: True labels\n",
        "    :returns: A scalar denoting correct prediction ratio\n",
        "    \"\"\"\n",
        "\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"\n",
        "    Convert a scipy sparse matrix to a torch sparse tensor.\n",
        "\n",
        "    :param sparse_mx: scipy sparse matrix to be converted to torch sparse tensor\n",
        "    :returns: A torch spare tensor\n",
        "    \"\"\"\n",
        "\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2wkxjDjkPNST"
      },
      "source": [
        "## Graph Convolutional Layer\n",
        "We implement simple Graph Convolutional Layer in this section. GraphConvolution is a class that extends `torch.nn.modules.module.Module` and therefore has a `forward` method that simulates a signle forward pass on the layer. Also note that in the `__init__` we initialize layer weights and optionally add a bias parameter to the layer. In the case of GraphConvolution layer, we have only a weight matrix $W$ where $ W \\in \\mathbb{R} ^ {I \\times O}$ where $I$ is the size of input features and $O$ is the size of output features. Formally output in each layer is calculated using equation below:<br><br>\n",
        "$h_l^{(i+1)} = \\sigma(\\sum\\limits_{j\\in\\mathcal{N(i)}}\\frac{1}{c_{ij}}W^{(l)}h_l^{(i)})$ \n",
        "<br>\n",
        "here $\\sigma$ is a non-linearity applied to the output (_Kipf et al._ used _ReLU_) and $c_{ij} = \\frac{1}{|\\mathcal{N(i)}||\\mathcal{N(j)}|}$ is a normalization coefficient that considers structure of neighborhood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XhAByHStPZrp"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gxrDpCzVPeUu"
      },
      "source": [
        "## Model\n",
        "Here we stack two `GraphConvolution` layers and build a 2-layer model of GCN. Since we have only 2 layers, the structure will have `nfeat` as input features size, `nhid` as hidden features size, and `nclass` as output size (number of classes). `GCN` also is a class that extends `Module` and has `forward` method to simulate forward pass.<br>\n",
        "Note that we implement non-linearity between layers here in the `GCN` class for the sake of simplicity, in case of higher number of layers, you may want to consider moving non-linearity applying term to the `GraphConvolution` class.<br>\n",
        "Input of the model will be input features `x` and adjacency matrix `adj` (plus structural settings `nfeat`, `nhid`, `ncalss` and `dropout`) and the output will be output of a `log_softmax` function applied on output features of 2nd layer (classification output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2gVGNZBEPi1w"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eUPU9gaEPtMe"
      },
      "source": [
        "## Train, Validate and Test\n",
        "Lastly after defining all modules we need, we load Cora dataset and instantiate our model. `args` object is created to set options of runtime and structure of our model. We use _Adam_ optimizer as it is used in the original paper, also if cuda is available and cuda option is set we make our tensors to use cuda.<br>\n",
        "Loss function used is _negative log likelihood_ as in the original paper.<br>\n",
        "In each epoch we pass all the nodes through the model and then calculate loss and accuracy for train and validation sets provided by train and validation masks `idx_train`, `idx_val`. Finally we test our model against test data provided by test mask `idx_test`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "EniVW8VVP-XF",
        "outputId": "0a22c590-33a4-425b-c7be-2f0f35181a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9341 acc_train: 0.0714 loss_val: 1.9239 acc_val: 0.1567 time: 0.1179s\n",
            "Epoch: 0002 loss_train: 1.9203 acc_train: 0.1857 loss_val: 1.9122 acc_val: 0.1567 time: 0.0091s\n",
            "Epoch: 0003 loss_train: 1.9074 acc_train: 0.1857 loss_val: 1.9008 acc_val: 0.1567 time: 0.0087s\n",
            "Epoch: 0004 loss_train: 1.9012 acc_train: 0.1929 loss_val: 1.8897 acc_val: 0.1567 time: 0.0104s\n",
            "Epoch: 0005 loss_train: 1.8863 acc_train: 0.2357 loss_val: 1.8790 acc_val: 0.1567 time: 0.0096s\n",
            "Epoch: 0006 loss_train: 1.8795 acc_train: 0.2071 loss_val: 1.8689 acc_val: 0.1600 time: 0.0097s\n",
            "Epoch: 0007 loss_train: 1.8556 acc_train: 0.2214 loss_val: 1.8594 acc_val: 0.1900 time: 0.0097s\n",
            "Epoch: 0008 loss_train: 1.8390 acc_train: 0.2929 loss_val: 1.8506 acc_val: 0.4000 time: 0.0095s\n",
            "Epoch: 0009 loss_train: 1.8410 acc_train: 0.3429 loss_val: 1.8421 acc_val: 0.4267 time: 0.0097s\n",
            "Epoch: 0010 loss_train: 1.8318 acc_train: 0.2786 loss_val: 1.8338 acc_val: 0.3833 time: 0.0093s\n",
            "Epoch: 0011 loss_train: 1.8338 acc_train: 0.3643 loss_val: 1.8256 acc_val: 0.3700 time: 0.0098s\n",
            "Epoch: 0012 loss_train: 1.8165 acc_train: 0.3000 loss_val: 1.8175 acc_val: 0.3567 time: 0.0094s\n",
            "Epoch: 0013 loss_train: 1.8108 acc_train: 0.3571 loss_val: 1.8095 acc_val: 0.3533 time: 0.0094s\n",
            "Epoch: 0014 loss_train: 1.8126 acc_train: 0.3071 loss_val: 1.8017 acc_val: 0.3500 time: 0.0094s\n",
            "Epoch: 0015 loss_train: 1.7948 acc_train: 0.3286 loss_val: 1.7939 acc_val: 0.3500 time: 0.0095s\n",
            "Epoch: 0016 loss_train: 1.7893 acc_train: 0.3071 loss_val: 1.7863 acc_val: 0.3500 time: 0.0097s\n",
            "Epoch: 0017 loss_train: 1.7827 acc_train: 0.3071 loss_val: 1.7788 acc_val: 0.3500 time: 0.0100s\n",
            "Epoch: 0018 loss_train: 1.7561 acc_train: 0.3000 loss_val: 1.7713 acc_val: 0.3500 time: 0.0098s\n",
            "Epoch: 0019 loss_train: 1.7624 acc_train: 0.2929 loss_val: 1.7641 acc_val: 0.3500 time: 0.0097s\n",
            "Epoch: 0020 loss_train: 1.7584 acc_train: 0.3000 loss_val: 1.7571 acc_val: 0.3500 time: 0.0148s\n",
            "Epoch: 0021 loss_train: 1.7318 acc_train: 0.3000 loss_val: 1.7502 acc_val: 0.3500 time: 0.0095s\n",
            "Epoch: 0022 loss_train: 1.7327 acc_train: 0.3000 loss_val: 1.7433 acc_val: 0.3500 time: 0.0093s\n",
            "Epoch: 0023 loss_train: 1.7167 acc_train: 0.3000 loss_val: 1.7366 acc_val: 0.3500 time: 0.0097s\n",
            "Epoch: 0024 loss_train: 1.7326 acc_train: 0.2929 loss_val: 1.7298 acc_val: 0.3500 time: 0.0093s\n",
            "Epoch: 0025 loss_train: 1.7377 acc_train: 0.2929 loss_val: 1.7231 acc_val: 0.3500 time: 0.0093s\n",
            "Epoch: 0026 loss_train: 1.7177 acc_train: 0.2929 loss_val: 1.7164 acc_val: 0.3500 time: 0.0108s\n",
            "Epoch: 0027 loss_train: 1.6894 acc_train: 0.3000 loss_val: 1.7099 acc_val: 0.3500 time: 0.0102s\n",
            "Epoch: 0028 loss_train: 1.6863 acc_train: 0.3071 loss_val: 1.7036 acc_val: 0.3500 time: 0.0085s\n",
            "Epoch: 0029 loss_train: 1.6917 acc_train: 0.2929 loss_val: 1.6972 acc_val: 0.3500 time: 0.0084s\n",
            "Epoch: 0030 loss_train: 1.6885 acc_train: 0.3000 loss_val: 1.6907 acc_val: 0.3500 time: 0.0086s\n",
            "Epoch: 0031 loss_train: 1.6694 acc_train: 0.2929 loss_val: 1.6842 acc_val: 0.3500 time: 0.0096s\n",
            "Epoch: 0032 loss_train: 1.6502 acc_train: 0.3143 loss_val: 1.6776 acc_val: 0.3500 time: 0.0083s\n",
            "Epoch: 0033 loss_train: 1.6471 acc_train: 0.3214 loss_val: 1.6709 acc_val: 0.3500 time: 0.0089s\n",
            "Epoch: 0034 loss_train: 1.6494 acc_train: 0.3214 loss_val: 1.6640 acc_val: 0.3500 time: 0.0085s\n",
            "Epoch: 0035 loss_train: 1.6288 acc_train: 0.3214 loss_val: 1.6570 acc_val: 0.3500 time: 0.0079s\n",
            "Epoch: 0036 loss_train: 1.6075 acc_train: 0.3286 loss_val: 1.6497 acc_val: 0.3533 time: 0.0084s\n",
            "Epoch: 0037 loss_train: 1.6126 acc_train: 0.3357 loss_val: 1.6422 acc_val: 0.3633 time: 0.0084s\n",
            "Epoch: 0038 loss_train: 1.5953 acc_train: 0.3571 loss_val: 1.6345 acc_val: 0.3633 time: 0.0081s\n",
            "Epoch: 0039 loss_train: 1.5703 acc_train: 0.3929 loss_val: 1.6266 acc_val: 0.3633 time: 0.0108s\n",
            "Epoch: 0040 loss_train: 1.5654 acc_train: 0.3714 loss_val: 1.6185 acc_val: 0.3700 time: 0.0107s\n",
            "Epoch: 0041 loss_train: 1.5582 acc_train: 0.3786 loss_val: 1.6101 acc_val: 0.3733 time: 0.0093s\n",
            "Epoch: 0042 loss_train: 1.5476 acc_train: 0.4286 loss_val: 1.6014 acc_val: 0.3800 time: 0.0101s\n",
            "Epoch: 0043 loss_train: 1.5262 acc_train: 0.4071 loss_val: 1.5922 acc_val: 0.3833 time: 0.0094s\n",
            "Epoch: 0044 loss_train: 1.5292 acc_train: 0.4286 loss_val: 1.5829 acc_val: 0.3833 time: 0.0101s\n",
            "Epoch: 0045 loss_train: 1.4934 acc_train: 0.4429 loss_val: 1.5731 acc_val: 0.3933 time: 0.0095s\n",
            "Epoch: 0046 loss_train: 1.4817 acc_train: 0.4357 loss_val: 1.5630 acc_val: 0.3967 time: 0.0096s\n",
            "Epoch: 0047 loss_train: 1.4991 acc_train: 0.4429 loss_val: 1.5526 acc_val: 0.4000 time: 0.0101s\n",
            "Epoch: 0048 loss_train: 1.4573 acc_train: 0.4429 loss_val: 1.5421 acc_val: 0.4067 time: 0.0091s\n",
            "Epoch: 0049 loss_train: 1.4987 acc_train: 0.3929 loss_val: 1.5315 acc_val: 0.4067 time: 0.0097s\n",
            "Epoch: 0050 loss_train: 1.4420 acc_train: 0.4500 loss_val: 1.5207 acc_val: 0.4100 time: 0.0106s\n",
            "Epoch: 0051 loss_train: 1.4372 acc_train: 0.4643 loss_val: 1.5100 acc_val: 0.4100 time: 0.0137s\n",
            "Epoch: 0052 loss_train: 1.4357 acc_train: 0.4571 loss_val: 1.4994 acc_val: 0.4167 time: 0.0119s\n",
            "Epoch: 0053 loss_train: 1.4020 acc_train: 0.4786 loss_val: 1.4888 acc_val: 0.4200 time: 0.0089s\n",
            "Epoch: 0054 loss_train: 1.4079 acc_train: 0.4571 loss_val: 1.4783 acc_val: 0.4233 time: 0.0083s\n",
            "Epoch: 0055 loss_train: 1.3643 acc_train: 0.4714 loss_val: 1.4678 acc_val: 0.4300 time: 0.0080s\n",
            "Epoch: 0056 loss_train: 1.3717 acc_train: 0.4786 loss_val: 1.4570 acc_val: 0.4333 time: 0.0078s\n",
            "Epoch: 0057 loss_train: 1.3274 acc_train: 0.5071 loss_val: 1.4457 acc_val: 0.4333 time: 0.0084s\n",
            "Epoch: 0058 loss_train: 1.3607 acc_train: 0.4714 loss_val: 1.4345 acc_val: 0.4433 time: 0.0090s\n",
            "Epoch: 0059 loss_train: 1.3116 acc_train: 0.4857 loss_val: 1.4232 acc_val: 0.4433 time: 0.0087s\n",
            "Epoch: 0060 loss_train: 1.2775 acc_train: 0.5143 loss_val: 1.4120 acc_val: 0.4533 time: 0.0082s\n",
            "Epoch: 0061 loss_train: 1.2964 acc_train: 0.4857 loss_val: 1.4008 acc_val: 0.4567 time: 0.0080s\n",
            "Epoch: 0062 loss_train: 1.2571 acc_train: 0.5286 loss_val: 1.3896 acc_val: 0.4600 time: 0.0088s\n",
            "Epoch: 0063 loss_train: 1.2621 acc_train: 0.5071 loss_val: 1.3786 acc_val: 0.4633 time: 0.0081s\n",
            "Epoch: 0064 loss_train: 1.2553 acc_train: 0.5429 loss_val: 1.3677 acc_val: 0.4733 time: 0.0080s\n",
            "Epoch: 0065 loss_train: 1.2270 acc_train: 0.5786 loss_val: 1.3568 acc_val: 0.4933 time: 0.0086s\n",
            "Epoch: 0066 loss_train: 1.2367 acc_train: 0.5643 loss_val: 1.3460 acc_val: 0.5033 time: 0.0081s\n",
            "Epoch: 0067 loss_train: 1.2142 acc_train: 0.5786 loss_val: 1.3353 acc_val: 0.5267 time: 0.0081s\n",
            "Epoch: 0068 loss_train: 1.1734 acc_train: 0.6000 loss_val: 1.3246 acc_val: 0.5433 time: 0.0079s\n",
            "Epoch: 0069 loss_train: 1.1762 acc_train: 0.6143 loss_val: 1.3138 acc_val: 0.5567 time: 0.0083s\n",
            "Epoch: 0070 loss_train: 1.1996 acc_train: 0.6357 loss_val: 1.3033 acc_val: 0.5900 time: 0.0081s\n",
            "Epoch: 0071 loss_train: 1.1538 acc_train: 0.6929 loss_val: 1.2929 acc_val: 0.6200 time: 0.0082s\n",
            "Epoch: 0072 loss_train: 1.1474 acc_train: 0.6857 loss_val: 1.2825 acc_val: 0.6333 time: 0.0084s\n",
            "Epoch: 0073 loss_train: 1.1592 acc_train: 0.7143 loss_val: 1.2719 acc_val: 0.6500 time: 0.0083s\n",
            "Epoch: 0074 loss_train: 1.1388 acc_train: 0.6357 loss_val: 1.2618 acc_val: 0.6700 time: 0.0080s\n",
            "Epoch: 0075 loss_train: 1.0988 acc_train: 0.6929 loss_val: 1.2520 acc_val: 0.6833 time: 0.0088s\n",
            "Epoch: 0076 loss_train: 1.0871 acc_train: 0.7286 loss_val: 1.2421 acc_val: 0.6800 time: 0.0079s\n",
            "Epoch: 0077 loss_train: 1.0591 acc_train: 0.7214 loss_val: 1.2322 acc_val: 0.6767 time: 0.0081s\n",
            "Epoch: 0078 loss_train: 1.0599 acc_train: 0.7714 loss_val: 1.2224 acc_val: 0.6867 time: 0.0083s\n",
            "Epoch: 0079 loss_train: 1.0916 acc_train: 0.7286 loss_val: 1.2125 acc_val: 0.6933 time: 0.0122s\n",
            "Epoch: 0080 loss_train: 1.0693 acc_train: 0.7286 loss_val: 1.2027 acc_val: 0.7033 time: 0.0092s\n",
            "Epoch: 0081 loss_train: 1.0354 acc_train: 0.7500 loss_val: 1.1929 acc_val: 0.7100 time: 0.0081s\n",
            "Epoch: 0082 loss_train: 1.0490 acc_train: 0.7714 loss_val: 1.1834 acc_val: 0.7200 time: 0.0081s\n",
            "Epoch: 0083 loss_train: 1.0136 acc_train: 0.7786 loss_val: 1.1743 acc_val: 0.7200 time: 0.0080s\n",
            "Epoch: 0084 loss_train: 1.0284 acc_train: 0.8000 loss_val: 1.1653 acc_val: 0.7267 time: 0.0078s\n",
            "Epoch: 0085 loss_train: 0.9774 acc_train: 0.8000 loss_val: 1.1569 acc_val: 0.7300 time: 0.0083s\n",
            "Epoch: 0086 loss_train: 1.0077 acc_train: 0.7643 loss_val: 1.1488 acc_val: 0.7467 time: 0.0087s\n",
            "Epoch: 0087 loss_train: 1.0293 acc_train: 0.7786 loss_val: 1.1407 acc_val: 0.7467 time: 0.0079s\n",
            "Epoch: 0088 loss_train: 0.9608 acc_train: 0.8214 loss_val: 1.1321 acc_val: 0.7500 time: 0.0078s\n",
            "Epoch: 0089 loss_train: 0.9627 acc_train: 0.7714 loss_val: 1.1233 acc_val: 0.7500 time: 0.0077s\n",
            "Epoch: 0090 loss_train: 0.9230 acc_train: 0.7929 loss_val: 1.1145 acc_val: 0.7500 time: 0.0078s\n",
            "Epoch: 0091 loss_train: 0.9201 acc_train: 0.8143 loss_val: 1.1062 acc_val: 0.7533 time: 0.0108s\n",
            "Epoch: 0092 loss_train: 0.9521 acc_train: 0.8000 loss_val: 1.0982 acc_val: 0.7533 time: 0.0085s\n",
            "Epoch: 0093 loss_train: 0.9093 acc_train: 0.8143 loss_val: 1.0909 acc_val: 0.7600 time: 0.0084s\n",
            "Epoch: 0094 loss_train: 0.9088 acc_train: 0.8071 loss_val: 1.0836 acc_val: 0.7667 time: 0.0088s\n",
            "Epoch: 0095 loss_train: 0.8930 acc_train: 0.8143 loss_val: 1.0762 acc_val: 0.7700 time: 0.0086s\n",
            "Epoch: 0096 loss_train: 0.9207 acc_train: 0.8214 loss_val: 1.0687 acc_val: 0.7733 time: 0.0100s\n",
            "Epoch: 0097 loss_train: 0.8853 acc_train: 0.8000 loss_val: 1.0618 acc_val: 0.7833 time: 0.0089s\n",
            "Epoch: 0098 loss_train: 0.8895 acc_train: 0.8071 loss_val: 1.0549 acc_val: 0.7833 time: 0.0084s\n",
            "Epoch: 0099 loss_train: 0.8601 acc_train: 0.8500 loss_val: 1.0484 acc_val: 0.7867 time: 0.0080s\n",
            "Epoch: 0100 loss_train: 0.8813 acc_train: 0.7929 loss_val: 1.0421 acc_val: 0.7867 time: 0.0087s\n",
            "Epoch: 0101 loss_train: 0.8872 acc_train: 0.8143 loss_val: 1.0360 acc_val: 0.7867 time: 0.0098s\n",
            "Epoch: 0102 loss_train: 0.8976 acc_train: 0.8000 loss_val: 1.0301 acc_val: 0.7800 time: 0.0084s\n",
            "Epoch: 0103 loss_train: 0.8423 acc_train: 0.8357 loss_val: 1.0246 acc_val: 0.7867 time: 0.0094s\n",
            "Epoch: 0104 loss_train: 0.8546 acc_train: 0.8071 loss_val: 1.0181 acc_val: 0.7933 time: 0.0081s\n",
            "Epoch: 0105 loss_train: 0.8408 acc_train: 0.8286 loss_val: 1.0111 acc_val: 0.7933 time: 0.0089s\n",
            "Epoch: 0106 loss_train: 0.8489 acc_train: 0.8429 loss_val: 1.0037 acc_val: 0.7933 time: 0.0081s\n",
            "Epoch: 0107 loss_train: 0.7965 acc_train: 0.8286 loss_val: 0.9962 acc_val: 0.7933 time: 0.0080s\n",
            "Epoch: 0108 loss_train: 0.8041 acc_train: 0.8286 loss_val: 0.9882 acc_val: 0.7967 time: 0.0083s\n",
            "Epoch: 0109 loss_train: 0.7945 acc_train: 0.8143 loss_val: 0.9808 acc_val: 0.8067 time: 0.0079s\n",
            "Epoch: 0110 loss_train: 0.7702 acc_train: 0.8357 loss_val: 0.9737 acc_val: 0.8033 time: 0.0081s\n",
            "Epoch: 0111 loss_train: 0.8179 acc_train: 0.8714 loss_val: 0.9675 acc_val: 0.8067 time: 0.0077s\n",
            "Epoch: 0112 loss_train: 0.7818 acc_train: 0.8500 loss_val: 0.9614 acc_val: 0.8033 time: 0.0076s\n",
            "Epoch: 0113 loss_train: 0.7904 acc_train: 0.8143 loss_val: 0.9554 acc_val: 0.8100 time: 0.0082s\n",
            "Epoch: 0114 loss_train: 0.7288 acc_train: 0.8357 loss_val: 0.9503 acc_val: 0.8100 time: 0.0081s\n",
            "Epoch: 0115 loss_train: 0.7508 acc_train: 0.8571 loss_val: 0.9443 acc_val: 0.8100 time: 0.0080s\n",
            "Epoch: 0116 loss_train: 0.7799 acc_train: 0.8143 loss_val: 0.9380 acc_val: 0.8067 time: 0.0080s\n",
            "Epoch: 0117 loss_train: 0.7687 acc_train: 0.8286 loss_val: 0.9319 acc_val: 0.8067 time: 0.0081s\n",
            "Epoch: 0118 loss_train: 0.7312 acc_train: 0.8714 loss_val: 0.9251 acc_val: 0.8100 time: 0.0081s\n",
            "Epoch: 0119 loss_train: 0.6964 acc_train: 0.8643 loss_val: 0.9184 acc_val: 0.8067 time: 0.0079s\n",
            "Epoch: 0120 loss_train: 0.6635 acc_train: 0.8786 loss_val: 0.9116 acc_val: 0.8100 time: 0.0082s\n",
            "Epoch: 0121 loss_train: 0.7357 acc_train: 0.8429 loss_val: 0.9053 acc_val: 0.8100 time: 0.0081s\n",
            "Epoch: 0122 loss_train: 0.7227 acc_train: 0.8357 loss_val: 0.8989 acc_val: 0.8100 time: 0.0099s\n",
            "Epoch: 0123 loss_train: 0.7163 acc_train: 0.8571 loss_val: 0.8930 acc_val: 0.8100 time: 0.0086s\n",
            "Epoch: 0124 loss_train: 0.6863 acc_train: 0.8500 loss_val: 0.8882 acc_val: 0.8100 time: 0.0080s\n",
            "Epoch: 0125 loss_train: 0.6699 acc_train: 0.8571 loss_val: 0.8838 acc_val: 0.8067 time: 0.0080s\n",
            "Epoch: 0126 loss_train: 0.6930 acc_train: 0.8500 loss_val: 0.8795 acc_val: 0.8100 time: 0.0082s\n",
            "Epoch: 0127 loss_train: 0.6791 acc_train: 0.8857 loss_val: 0.8757 acc_val: 0.8100 time: 0.0083s\n",
            "Epoch: 0128 loss_train: 0.6882 acc_train: 0.8643 loss_val: 0.8718 acc_val: 0.8100 time: 0.0091s\n",
            "Epoch: 0129 loss_train: 0.6530 acc_train: 0.8857 loss_val: 0.8673 acc_val: 0.8100 time: 0.0094s\n",
            "Epoch: 0130 loss_train: 0.6394 acc_train: 0.8357 loss_val: 0.8629 acc_val: 0.8100 time: 0.0100s\n",
            "Epoch: 0131 loss_train: 0.6616 acc_train: 0.8786 loss_val: 0.8580 acc_val: 0.8100 time: 0.0092s\n",
            "Epoch: 0132 loss_train: 0.6337 acc_train: 0.8571 loss_val: 0.8533 acc_val: 0.8100 time: 0.0096s\n",
            "Epoch: 0133 loss_train: 0.6280 acc_train: 0.8714 loss_val: 0.8483 acc_val: 0.8100 time: 0.0093s\n",
            "Epoch: 0134 loss_train: 0.6519 acc_train: 0.8143 loss_val: 0.8437 acc_val: 0.8100 time: 0.0103s\n",
            "Epoch: 0135 loss_train: 0.6596 acc_train: 0.8643 loss_val: 0.8399 acc_val: 0.8067 time: 0.0105s\n",
            "Epoch: 0136 loss_train: 0.6427 acc_train: 0.8786 loss_val: 0.8370 acc_val: 0.8100 time: 0.0098s\n",
            "Epoch: 0137 loss_train: 0.6405 acc_train: 0.8357 loss_val: 0.8343 acc_val: 0.8100 time: 0.0099s\n",
            "Epoch: 0138 loss_train: 0.6075 acc_train: 0.8786 loss_val: 0.8326 acc_val: 0.8067 time: 0.0096s\n",
            "Epoch: 0139 loss_train: 0.6089 acc_train: 0.8714 loss_val: 0.8313 acc_val: 0.8067 time: 0.0105s\n",
            "Epoch: 0140 loss_train: 0.6276 acc_train: 0.8786 loss_val: 0.8299 acc_val: 0.8033 time: 0.0096s\n",
            "Epoch: 0141 loss_train: 0.6165 acc_train: 0.8429 loss_val: 0.8281 acc_val: 0.8033 time: 0.0084s\n",
            "Epoch: 0142 loss_train: 0.5885 acc_train: 0.8857 loss_val: 0.8257 acc_val: 0.8067 time: 0.0110s\n",
            "Epoch: 0143 loss_train: 0.5822 acc_train: 0.8857 loss_val: 0.8216 acc_val: 0.8067 time: 0.0086s\n",
            "Epoch: 0144 loss_train: 0.6439 acc_train: 0.8286 loss_val: 0.8159 acc_val: 0.8100 time: 0.0080s\n",
            "Epoch: 0145 loss_train: 0.5974 acc_train: 0.8571 loss_val: 0.8104 acc_val: 0.8100 time: 0.0081s\n",
            "Epoch: 0146 loss_train: 0.5738 acc_train: 0.9214 loss_val: 0.8058 acc_val: 0.8133 time: 0.0081s\n",
            "Epoch: 0147 loss_train: 0.6067 acc_train: 0.8643 loss_val: 0.8026 acc_val: 0.8133 time: 0.0079s\n",
            "Epoch: 0148 loss_train: 0.5565 acc_train: 0.8929 loss_val: 0.7992 acc_val: 0.8133 time: 0.0079s\n",
            "Epoch: 0149 loss_train: 0.6066 acc_train: 0.8929 loss_val: 0.7958 acc_val: 0.8167 time: 0.0081s\n",
            "Epoch: 0150 loss_train: 0.5636 acc_train: 0.8786 loss_val: 0.7925 acc_val: 0.8133 time: 0.0079s\n",
            "Epoch: 0151 loss_train: 0.5882 acc_train: 0.9071 loss_val: 0.7904 acc_val: 0.8133 time: 0.0080s\n",
            "Epoch: 0152 loss_train: 0.5484 acc_train: 0.8929 loss_val: 0.7888 acc_val: 0.8133 time: 0.0077s\n",
            "Epoch: 0153 loss_train: 0.5346 acc_train: 0.9000 loss_val: 0.7880 acc_val: 0.8067 time: 0.0080s\n",
            "Epoch: 0154 loss_train: 0.5679 acc_train: 0.8714 loss_val: 0.7874 acc_val: 0.8133 time: 0.0080s\n",
            "Epoch: 0155 loss_train: 0.5917 acc_train: 0.8429 loss_val: 0.7858 acc_val: 0.8133 time: 0.0114s\n",
            "Epoch: 0156 loss_train: 0.5326 acc_train: 0.9071 loss_val: 0.7837 acc_val: 0.8133 time: 0.0112s\n",
            "Epoch: 0157 loss_train: 0.5727 acc_train: 0.8429 loss_val: 0.7821 acc_val: 0.8133 time: 0.0083s\n",
            "Epoch: 0158 loss_train: 0.5203 acc_train: 0.9071 loss_val: 0.7803 acc_val: 0.8133 time: 0.0077s\n",
            "Epoch: 0159 loss_train: 0.5571 acc_train: 0.8786 loss_val: 0.7776 acc_val: 0.8133 time: 0.0077s\n",
            "Epoch: 0160 loss_train: 0.5455 acc_train: 0.9000 loss_val: 0.7749 acc_val: 0.8133 time: 0.0078s\n",
            "Epoch: 0161 loss_train: 0.5321 acc_train: 0.8714 loss_val: 0.7719 acc_val: 0.8167 time: 0.0077s\n",
            "Epoch: 0162 loss_train: 0.5243 acc_train: 0.8786 loss_val: 0.7700 acc_val: 0.8067 time: 0.0078s\n",
            "Epoch: 0163 loss_train: 0.5322 acc_train: 0.8929 loss_val: 0.7679 acc_val: 0.8067 time: 0.0113s\n",
            "Epoch: 0164 loss_train: 0.5413 acc_train: 0.8786 loss_val: 0.7658 acc_val: 0.8067 time: 0.0078s\n",
            "Epoch: 0165 loss_train: 0.5271 acc_train: 0.9143 loss_val: 0.7631 acc_val: 0.8133 time: 0.0079s\n",
            "Epoch: 0166 loss_train: 0.5217 acc_train: 0.9143 loss_val: 0.7607 acc_val: 0.8100 time: 0.0094s\n",
            "Epoch: 0167 loss_train: 0.5270 acc_train: 0.8857 loss_val: 0.7597 acc_val: 0.8100 time: 0.0078s\n",
            "Epoch: 0168 loss_train: 0.5358 acc_train: 0.9071 loss_val: 0.7581 acc_val: 0.8100 time: 0.0092s\n",
            "Epoch: 0169 loss_train: 0.4674 acc_train: 0.9071 loss_val: 0.7562 acc_val: 0.8133 time: 0.0079s\n",
            "Epoch: 0170 loss_train: 0.5070 acc_train: 0.9071 loss_val: 0.7542 acc_val: 0.8133 time: 0.0085s\n",
            "Epoch: 0171 loss_train: 0.4837 acc_train: 0.9286 loss_val: 0.7515 acc_val: 0.8133 time: 0.0091s\n",
            "Epoch: 0172 loss_train: 0.4969 acc_train: 0.8857 loss_val: 0.7490 acc_val: 0.8100 time: 0.0079s\n",
            "Epoch: 0173 loss_train: 0.5014 acc_train: 0.9071 loss_val: 0.7471 acc_val: 0.8100 time: 0.0080s\n",
            "Epoch: 0174 loss_train: 0.5034 acc_train: 0.9071 loss_val: 0.7458 acc_val: 0.8100 time: 0.0079s\n",
            "Epoch: 0175 loss_train: 0.5556 acc_train: 0.9000 loss_val: 0.7454 acc_val: 0.8067 time: 0.0084s\n",
            "Epoch: 0176 loss_train: 0.4925 acc_train: 0.9071 loss_val: 0.7453 acc_val: 0.8067 time: 0.0079s\n",
            "Epoch: 0177 loss_train: 0.4718 acc_train: 0.9143 loss_val: 0.7456 acc_val: 0.8100 time: 0.0080s\n",
            "Epoch: 0178 loss_train: 0.4938 acc_train: 0.9000 loss_val: 0.7456 acc_val: 0.8100 time: 0.0080s\n",
            "Epoch: 0179 loss_train: 0.4973 acc_train: 0.8929 loss_val: 0.7454 acc_val: 0.8100 time: 0.0079s\n",
            "Epoch: 0180 loss_train: 0.4970 acc_train: 0.9214 loss_val: 0.7448 acc_val: 0.8100 time: 0.0087s\n",
            "Epoch: 0181 loss_train: 0.4877 acc_train: 0.9071 loss_val: 0.7430 acc_val: 0.8100 time: 0.0085s\n",
            "Epoch: 0182 loss_train: 0.4743 acc_train: 0.9071 loss_val: 0.7407 acc_val: 0.8100 time: 0.0090s\n",
            "Epoch: 0183 loss_train: 0.4671 acc_train: 0.9357 loss_val: 0.7360 acc_val: 0.8133 time: 0.0093s\n",
            "Epoch: 0184 loss_train: 0.4665 acc_train: 0.8857 loss_val: 0.7321 acc_val: 0.8167 time: 0.0084s\n",
            "Epoch: 0185 loss_train: 0.4963 acc_train: 0.8857 loss_val: 0.7291 acc_val: 0.8167 time: 0.0091s\n",
            "Epoch: 0186 loss_train: 0.4819 acc_train: 0.8929 loss_val: 0.7267 acc_val: 0.8200 time: 0.0085s\n",
            "Epoch: 0187 loss_train: 0.4794 acc_train: 0.9143 loss_val: 0.7250 acc_val: 0.8133 time: 0.0078s\n",
            "Epoch: 0188 loss_train: 0.4484 acc_train: 0.9500 loss_val: 0.7235 acc_val: 0.8133 time: 0.0079s\n",
            "Epoch: 0189 loss_train: 0.4850 acc_train: 0.8929 loss_val: 0.7226 acc_val: 0.8133 time: 0.0090s\n",
            "Epoch: 0190 loss_train: 0.4423 acc_train: 0.9071 loss_val: 0.7219 acc_val: 0.8167 time: 0.0085s\n",
            "Epoch: 0191 loss_train: 0.4809 acc_train: 0.9214 loss_val: 0.7212 acc_val: 0.8133 time: 0.0082s\n",
            "Epoch: 0192 loss_train: 0.4563 acc_train: 0.9000 loss_val: 0.7208 acc_val: 0.8133 time: 0.0083s\n",
            "Epoch: 0193 loss_train: 0.4619 acc_train: 0.9143 loss_val: 0.7197 acc_val: 0.8133 time: 0.0079s\n",
            "Epoch: 0194 loss_train: 0.4655 acc_train: 0.9571 loss_val: 0.7180 acc_val: 0.8133 time: 0.0079s\n",
            "Epoch: 0195 loss_train: 0.4479 acc_train: 0.9500 loss_val: 0.7164 acc_val: 0.8133 time: 0.0080s\n",
            "Epoch: 0196 loss_train: 0.4533 acc_train: 0.9071 loss_val: 0.7149 acc_val: 0.8133 time: 0.0094s\n",
            "Epoch: 0197 loss_train: 0.4727 acc_train: 0.9071 loss_val: 0.7139 acc_val: 0.8133 time: 0.0094s\n",
            "Epoch: 0198 loss_train: 0.4679 acc_train: 0.9214 loss_val: 0.7128 acc_val: 0.8167 time: 0.0100s\n",
            "Epoch: 0199 loss_train: 0.4242 acc_train: 0.9214 loss_val: 0.7119 acc_val: 0.8167 time: 0.0093s\n",
            "Epoch: 0200 loss_train: 0.4324 acc_train: 0.9071 loss_val: 0.7119 acc_val: 0.8133 time: 0.0094s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2.1178s\n",
            "Test set results: loss= 0.7439 accuracy= 0.8320\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "# Training settings\n",
        "\n",
        "ARGS = namedtuple('ARGS', ['cuda', 'fastmode', 'seed', 'epochs', 'lr',\n",
        "                           'weight_decay', 'hidden', 'dropout'])\n",
        "\n",
        "args = ARGS(cuda=True, fastmode=False, seed=42, epochs=200, lr=0.01,\n",
        "            weight_decay=5e-4, hidden=16, dropout=0.5)\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=args.dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if not args.fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Testing\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Visualization\n",
        "Here we visualize the change of loss and accuracy during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}